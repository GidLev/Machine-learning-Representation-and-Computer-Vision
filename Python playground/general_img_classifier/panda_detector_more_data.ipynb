{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV VERSION (should be 3.1.0 or later, with nonfree modules installed!): 3.1.0-dev\n",
      "Scoring grid search with metric: f1_micro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import visual_bow as bow\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.externals import joblib\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "SCORING = 'f1_micro'\n",
    "print 'Scoring grid search with metric: %s' % SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8639 total negative imgs to choose from\n",
      "[('101_ObjectCategories\\\\accordion\\\\image_0001.jpg', False), ('101_ObjectCategories\\\\accordion\\\\image_0002.jpg', False), ('101_ObjectCategories\\\\accordion\\\\image_0003.jpg', False), ('101_ObjectCategories\\\\accordion\\\\image_0004.jpg', False), ('101_ObjectCategories\\\\accordion\\\\image_0005.jpg', False)]\n"
     ]
    }
   ],
   "source": [
    "# Get all possible negative images and label them False\n",
    "positive_folder='panda'\n",
    "all_negs = [(path, False) for path in bow.neg_img_cal101(positive_folder)]\n",
    "print '%i total negative imgs to choose from' % len(all_negs)\n",
    "print all_negs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 positive images\n",
      "[('panda_rip\\\\image_0001.jpg', True), ('panda_rip\\\\image_0002.jpg', True), ('panda_rip\\\\image_0003.jpg', True), ('panda_rip\\\\image_0004.jpg', True), ('panda_rip\\\\image_0005.jpg', True)]\n"
     ]
    }
   ],
   "source": [
    "# Get all the positive images you have (in the panda_rip folder) and label them True\n",
    "positive_imgs = [(path, True) for path in glob.glob('panda_rip/*')]\n",
    "print '%i positive images' % len(positive_imgs)\n",
    "print positive_imgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 total images (1:1 positive:negative)\n",
      "[('101_ObjectCategories\\\\revolver\\\\image_0028.jpg', False), ('panda_rip\\\\image_0022.jpg', True), ('panda_rip\\\\image_0009.jpg', True), ('panda_rip\\\\image_0004.jpg', True), ('101_ObjectCategories\\\\airplanes\\\\image_0235.jpg', False)]\n"
     ]
    }
   ],
   "source": [
    "# take N random negative images, where N is no of positive images\n",
    "# then concatenate N pos + N neg and shuffle.\n",
    "chosen_negs = random.sample(all_negs, len(positive_imgs))\n",
    "imgs = chosen_negs + positive_imgs\n",
    "\n",
    "np.random.shuffle(imgs)\n",
    "\n",
    "print '%i total images (1:1 positive:negative)' % len(imgs)\n",
    "print imgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating SIFT descriptors for 76 images\n",
      "SIFT descriptors generated.\n",
      "Wall time: 4.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "img_descs, y = bow.gen_sift_features(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickles/img_descs/y.pickle', 'pickles/img_descs/y.pickle_01.npy']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(img_descs, 'pickles/img_descs/img_descs.pickle')\n",
    "# joblib.dump(y, 'pickles/img_descs/y.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test-val split: 54 training rows, 11 test rows, 11 validation rows\n"
     ]
    }
   ],
   "source": [
    "# generate indexes for train/test/val split\n",
    "training_idxs, test_idxs, val_idxs = bow.train_test_val_split_idxs(\n",
    "    total_rows=len(imgs), \n",
    "    percent_test=0.15, \n",
    "    percent_val=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the SIFT descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23027 descriptors before clustering\n",
      "Using clustering model MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10,\n",
      "        n_clusters=250, n_init=3, random_state=None,\n",
      "        reassignment_ratio=0.01, tol=0.0, verbose=0)...\n",
      "Clustering on training set to get codebook of 250 words\n",
      "done clustering. Using clustering model to generate BoW histograms for each image.\n",
      "done generating BoW histograms.\n",
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "K_CLUSTERS = 250\n",
    "\n",
    "# MiniBatchKMeans annoyingly throws tons of deprecation warnings that fill up the notebook. Ignore them.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X, cluster_model = bow.cluster_features(\n",
    "    img_descs, \n",
    "    training_idxs=training_idxs, \n",
    "    cluster_model=MiniBatchKMeans(n_clusters=K_CLUSTERS)\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = bow.perform_data_split(X, y, training_idxs, test_idxs, val_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment to pickle the clustered Visual BoW features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for obj, obj_name in zip( [X_train, X_test, X_val, y_train, y_test, y_val], \n",
    "#                          ['X_train', 'X_test', 'X_val', 'y_train', 'y_test', 'y_val'] ):\n",
    "#     joblib.dump(obj, 'pickles/feature_data/%s.pickle' % obj_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment to LOAD pickle of clustered Visual BoW features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for obj_name in ['X_train', 'X_test', 'X_val', 'y_train', 'y_test', 'y_val']:\n",
    "#     exec(\"{obj_name} = joblib.load('pickles/feature_data/{obj_name}.pickle')\".format(obj_name=obj_name))\n",
    "#     exec(\"print obj_name, len({0})\".format(obj_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score (f1_micro): 1.0\n",
      "test score (f1_micro): 0.727272727273\n",
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Wall time: 3.43 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:426: ChangedBehaviorWarning: The long-standing behavior to use the estimator's score function in GridSearchCV.score has changed. The scoring parameter is now used.\n",
      "  ChangedBehaviorWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# c_vals = [0.0001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "c_vals = [0.1, 1, 5, 10]\n",
    "# c_vals = [1]\n",
    "\n",
    "gamma_vals = [0.5, 0.1, 0.01, 0.0001, 0.00001]\n",
    "# gamma_vals = [0.5, 0.1]\n",
    "# gamma_vals = [0.1]\n",
    "\n",
    "param_grid = [\n",
    "  {'C': c_vals, 'kernel': ['linear']},\n",
    "  {'C': c_vals, 'gamma': gamma_vals, 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "svc = GridSearchCV(SVC(), param_grid, n_jobs=-1, scoring=SCORING)\n",
    "svc.fit(X_train, y_train)\n",
    "print 'train score (%s):'%SCORING, svc.score(X_train, y_train)\n",
    "print 'test score (%s):'%SCORING, svc.score(X_test, y_test)\n",
    "\n",
    "print svc.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have our estimator, this is how it could classify random pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101_ObjectCategories\\Faces_easy\\image_0316.jpg ['False']\n",
      "101_ObjectCategories\\car_side\\image_0093.jpg ['False']\n",
      "101_ObjectCategories\\garfield\\image_0002.jpg ['False']\n",
      "101_ObjectCategories\\Motorbikes\\image_0280.jpg ['False']\n",
      "101_ObjectCategories\\hawksbill\\image_0012.jpg ['True']\n",
      "101_ObjectCategories\\revolver\\image_0018.jpg ['False']\n",
      "101_ObjectCategories\\airplanes\\image_0139.jpg ['False']\n",
      "101_ObjectCategories\\Motorbikes\\image_0117.jpg ['False']\n",
      "101_ObjectCategories\\airplanes\\image_0215.jpg ['False']\n",
      "101_ObjectCategories\\brain\\image_0027.jpg ['False']\n"
     ]
    }
   ],
   "source": [
    "for img_path, label in random.sample(all_negs, 10):\n",
    "    print img_path, svc.predict(bow.img_to_vect(img_path, cluster_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment to pickle the best SVC classifier & kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# joblib.dump(svc.best_estimator_, 'pickles/svc/svc.pickle')\n",
    "# joblib.dump(cluster_model, 'pickles/cluster_model/cluster_model.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Try AdaBoost, it's a common choice for SIFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score (f1_micro): 1.0\n",
      "test score (f1_micro): 0.727272727273\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.9, n_estimators=250, random_state=None)\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ada_params = {\n",
    "    'n_estimators':[100, 250, 500, 750],\n",
    "    'learning_rate':[0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "}\n",
    "\n",
    "# ada = AdaBoostClassifier(n_estimators=MAX_ESTIMATORS, learning_rate=0.8)\n",
    "ada = GridSearchCV(AdaBoostClassifier(), ada_params, n_jobs=-1, scoring=SCORING)\n",
    "ada.fit(X_train, y_train)\n",
    "print 'train score (%s):'%SCORING, ada.score(X_train, y_train)\n",
    "print 'test score (%s):'%SCORING, ada.score(X_test, y_test)\n",
    "print ada.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment to pickle the AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# joblib.dump(ada.best_estimator_, 'pickles/ada/ada.pickle');\n",
    "# print 'picked adaboost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Separate out the clustering from the feature generation. They should be 2 different functions, the clustering should take the SIFT **training** data as an argument. It has labels already, right? Then you can save the SIFT data before clustering. Finally, you can do a grid search across K_CLUSTERS.\n",
    "\n",
    "* Also it would be cool to graph the above."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
